{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGB #Sample code\n",
    "\n",
    "A neural network is a computational tool that has to ability to easily learn complex patterns from a large quantity of data for applications such as classification and feature imitation.\n",
    "\n",
    "A neural network consists of input nodes and subsequent nodes that sum the value of nodes before it, with each node value multiplied by a scalar. Initially, the scalar values are randomly assigned. But a partial derivative can determine the influence of each node on the final output. The values of the scalars are then updated based on the influence/partial-derivative values and the sizes of the classification errors. Over time, this gradient descent process would improve the classification accuracy. With sufficient training data, neural networks can then be used to identify scat samples in the wild based on their reflectance spectra.\n",
    "\n",
    "Train your own neural network by selecting the \"Kernel\" at the top and click \"Restart Kernel and Run All Cells...\"\n",
    "\n",
    "<img src=\"../Image/Instruction.PNG\" />\n",
    "\n",
    "\n",
    "### Additional resources:\n",
    "\n",
    "* Click [here](InteractiveDisplay.ipynb) for an interactive display of the neural network result\n",
    "* Here are links to the 10 neural networks used in the experiment:\n",
    "    *   Hyperspectral\n",
    "      *   [Trial 1](Hyperspectral1.ipynb)\n",
    "      *   [Trial 2](Hyperspectral2.ipynb)\n",
    "      *   [Trial 3](Hyperspectral3.ipynb)\n",
    "      *   [Trial 4](Hyperspectral4.ipynb)\n",
    "      *   [Trial 5](Hyperspectral5.ipynb)\n",
    "    *   RGB\n",
    "      *   [Trial 1](RGB1.ipynb)\n",
    "      *   [Trial 2](RGB2.ipynb)\n",
    "      *   [Trial 3](RGB3.ipynb)\n",
    "      *   [Trial 4](RGB4.ipynb)\n",
    "      *   [Trial 5](RGB5.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "## Setting up\n",
    "\n",
    "#### 1. Specifying the RGB Network specific variables\n",
    "Other than the variable in the next cell, the rest of the code is common for both a RGB and hyperspectral network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = '../training_rgb_sample'\n",
    "data_path =\"../Training data/RGB.csv\"\n",
    "n_o_input = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Importing Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randrange\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "#### 3. Importing data and splitting it into label and training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path)\n",
    "\n",
    "data.head()\n",
    "\n",
    "target_fields ='Class code'\n",
    "data = data.drop([\"Class\",\"Sample\",\"Session.Sample\"],axis=1)\n",
    "features0, targets0 = data.drop(target_fields, axis=1), data[target_fields]\n",
    "features, targets  = np.array(features0) , np.array(targets0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Function to generate a non-repeating list of random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_no_repeat(lower,upper,length):\n",
    "    assert length<=(upper-lower)+1\n",
    "    output = []\n",
    "    while len(output)<length:\n",
    "        x = random.randint(lower,upper)\n",
    "        if not(x in output):\n",
    "            output.append(x)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Pick spectral data from random sessions for testing and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_length = 1\n",
    "test_length = 1\n",
    "n_o_sample = 47\n",
    "size_of_a_sample = 7\n",
    "\n",
    "#Pick random labels for validation & test set\n",
    "val_labels = []\n",
    "test_labels = [] \n",
    "for num in range(n_o_sample):\n",
    "    rand_upper = size_of_a_sample-1\n",
    "    rand_len = test_length+validation_length\n",
    "    ran_list = random_no_repeat(0,rand_upper,rand_len)\n",
    "    \n",
    "    rand_valid = np.array(ran_list[0:validation_length])\n",
    "    val_labels.extend(n_o_sample*rand_valid+num)\n",
    "    \n",
    "    rand_t_start = validation_length\n",
    "    rand_t_end = validation_length+test_length\n",
    "    rand_test= np.array(ran_list[rand_t_start:rand_t_end])\n",
    "    test_labels.extend(n_o_sample*rand_test+num)\n",
    "\n",
    "#Labels for training set by removing those allocated to test & validation\n",
    "train_labels = list(\n",
    "    set([x for x in range(n_o_sample*size_of_a_sample)])\n",
    "    - set(val_labels)\n",
    "    - set(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 1\n",
    "Check that the random session index for each sample is pick correctly, where labels of validation and test set should be same for given sample number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[141] [282]\n",
      "[1] [1]\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "print(num) #Sample number\n",
    "print(n_o_sample*rand_valid+num,n_o_sample*rand_test+num) #Index of sample from random trials\n",
    "print(targets[n_o_sample*rand_valid+num],targets[n_o_sample*rand_test+num]) #Lables of random samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. The samples are ordered in related groups. Randomizing the data would remove biases from the sample's related order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffle(train_labels)\n",
    "shuffle(val_labels)\n",
    "shuffle(test_labels)\n",
    "train_x, train_y = features[train_labels] ,targets[train_labels]\n",
    "val_x , val_y = features[val_labels] , targets[val_labels]\n",
    "test_x, test_y = features[test_labels] ,targets[test_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 2\n",
    "Check that labels are in same order (disable the shuffle function first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 8 1 1 1 3 6 3 3 4 5 1 1 3 5 2 3 8 2 2 5 4 4 1 2 3 4 2 2 2 3 7 1 5 3 2 5\n",
      " 4 3 3 2 3 2 2 6 7 2]\n",
      "[2 2 3 2 4 1 1 4 4 3 2 1 7 4 3 3 8 3 2 1 2 8 3 5 5 6 5 3 2 3 2 3 3 2 2 1 5\n",
      " 1 1 2 5 3 7 4 6 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(test_y)\n",
    "print(val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structuring the networks\n",
    "#### 7. The 1st layer of the hyperspectral network\n",
    "For the hyperspectral data set, an additional layer was added to the start of the network topology to reduce the dataâ€™s complexity. Traditionally, the first hidden layer looks for simpler features, in this case, the individual absorption features. Looking for correlation at an individual wavelength level, however, requires much computational cost. But as the absorption features exist as clusters of neighbouring wavelengths, we can use a convolution network like approach to convey the sequential and incremental relationship of wavelength inputs.  The prioritization of relationships between neighbouring inputs would reduce the computational cost. This convolution network like structures was, however, built from scratch, by stacking smaller dense layers together. This was to avoid the spatial invariance, whose generalisation in absorption features characteristics would disregard their potential variance across the spectrum.\n",
    "\n",
    "[simplify elaboration]\n",
    "\n",
    "[Diagram]\n",
    "\n",
    "\n",
    "A leaky RELU (Rectified linear unit) was also used in both networks as the activation function. This ensured the required non-linearity for problem-solving without the high computational cost of functions like softmax. It also prevented the exploding gradient in backpropagation that is associated with the conventional RELU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperspectral(input_,n_o_input, keep_prob, filter_width = 1, stride_size =1, relu_alpha = 0.2):\n",
    "    n_o_strides = int((n_o_input-filter_width)/stride_size) +1  #round down\n",
    "   \n",
    "    Hyper_layer = []\n",
    "    \n",
    "    def dense(input_,start,keep_prob, filter_width, stride_size, relu_alpha):\n",
    "        nn_input = tf.slice(input_,[0,start],[-1,filter_width])\n",
    "        \n",
    "        dropout1 = tf.nn.dropout(nn_input, keep_prob)\n",
    "        dense1 = tf.layers.dense(dropout1, 1)\n",
    "        relu1 = tf.maximum(relu_alpha * dense1, dense1)        \n",
    "        return relu1\n",
    "    \n",
    "    for step in range(n_o_strides):\n",
    "        start = step*stride_size\n",
    "        output = dense(input_,start,keep_prob, filter_width, stride_size, relu_alpha)\n",
    "        Hyper_layer.append(output)\n",
    "    \n",
    "    if (n_o_input-filter_width)%stride_size>0:\n",
    "        start = n_o_input-filter_width\n",
    "        output = dense(input_,start,keep_prob, filter_width, stride_size, relu_alpha)\n",
    "        Hyper_layer.append(output)\n",
    "        \n",
    "    Hyper_l_stacked = tf.concat(Hyper_layer,1)\n",
    "    \n",
    "    print(\"Hyper_l_stacked\",Hyper_l_stacked)\n",
    "    return Hyper_l_stacked , n_o_strides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. The remaining layers which are common for both the hyperspectral and RGB network\n",
    "Consisting of 2 layers, the remaining structures for the neural networks of the RGB and hyperspectral data set were similar. Both outputs were then consisted of 8 nodes, for the 8 total sub-classes of dropping and abiotic elements. For the droppings, the classes were segregated by the 5 type of excrement and the animal of origin. For the abiotic element, the classes were instead segregated by 3 material types. [See table ... from the report] For the 2 networks, the sizes of the hidden layer prior to the outputs were then both calculated by multiplying the layer prior with a â…” ratio, a structure that is common in neural network development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Classifier(input_,n_o_class,n_o_input, keep_prob,relu_alpha = 0.2):\n",
    "    print(\"n_o_input\",n_o_input)\n",
    "    if n_o_input == 3:\n",
    "        is_RGB = True\n",
    "    elif n_o_input == 571:\n",
    "        is_RGB = False\n",
    "    else:\n",
    "        raise ValueError('A very specific bad thing happened.'+str(n_o_input))\n",
    "    \n",
    "    if is_RGB:\n",
    "        dense0 = tf.layers.dense(input_, 3)    \n",
    "        relu0 = tf.maximum(relu_alpha * dense0, dense0)\n",
    "        first_layer_out = tf.nn.dropout(relu0, keep_prob)\n",
    "    else:\n",
    "        first_layer_out,n_o_input= hyperspectral(input_,n_o_input, keep_prob, filter_width = 30, stride_size =1, relu_alpha = 0.2)\n",
    "\n",
    "    hidden_size = n_o_input*2/3\n",
    "    hidden_nodes = int(hidden_size)+1 # rounding\n",
    "    print(\"hidden size:\",str(hidden_nodes))\n",
    "    \n",
    "    \n",
    "    dense1 = tf.layers.dense(first_layer_out, hidden_nodes)    \n",
    "    relu1 = tf.maximum(relu_alpha * dense1, dense1)\n",
    "    dropout1 = tf.nn.dropout(relu1, keep_prob)\n",
    "    \n",
    "    \n",
    "    class_logits = tf.layers.dense(dropout1, n_o_class)    \n",
    "    \n",
    "    return class_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up for training\n",
    "#### 9. Function to format neural network input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_indv(x,n_o_class):    \n",
    "    output = np.zeros(n_o_class)\n",
    "    output[x-1]=1\n",
    "    return output\n",
    "\n",
    "def one_hot_encode(x,n_o_class):\n",
    "    output = []\n",
    "    for y in x:\n",
    "        output.append(one_hot_indv(y,n_o_class))\n",
    "        \n",
    "    return np.array(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Function to split data into smaller batch for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y,batch_size=10):\n",
    "    n_batches = len(x)//batch_size\n",
    "    \n",
    "    for ii in range(0, n_batches*batch_size, batch_size):\n",
    "        # If we're not on the last batch, grab data with size batch_size\n",
    "        if ii != (n_batches-1)*batch_size:\n",
    "            X, Y = x[ii: ii+batch_size], y[ii: ii+batch_size] \n",
    "        # On the last batch, grab the rest of the data\n",
    "        else:\n",
    "            X, Y = x[ii:], y[ii:]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Function to define loss value for training\n",
    "Inspired by the aggregate loss scoring system from a GAN semi-supervised networ, an aggregated scoring system was structured to calculate the networkâ€™s loss value for training. This averaged the loss between the main class classification and the subclass classification; the main classes being manure vs non-manure, and the subclasses being the specific animal class or material type. And this aggregated scoring system aimed to teach the network the relationship between the groups of subclasses. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_loss(input_,target_,m_class, n_class,n_o_input, keep_prob,relu_alpha = 0.2,sub_scaling = 1):\n",
    "    \n",
    "    n_o_class = m_class+n_class\n",
    "    \n",
    "    #raw output\n",
    "    logits= Classifier(input_,n_o_class,n_o_input, keep_prob,relu_alpha = 0.2)\n",
    "    subclass_softmax = tf.nn.softmax(logits)\n",
    "    \n",
    "    #Reduce outputs from 8 subclasses to 2 main classes\n",
    "    n_class_logit, m_class_logit = tf.split(logits, [n_class, m_class], 1)\n",
    "    m_class_logit1 =tf.reduce_sum(m_class_logit,1, keepdims =True) \n",
    "    n_class_logit1 =tf.reduce_sum(n_class_logit,1, keepdims =True) \n",
    "    main_class_logits = tf.concat([n_class_logit1, m_class_logit1], 1)\n",
    "    main_class_softmax = tf.nn.softmax(main_class_logits)\n",
    "    \n",
    "    #Reduce labels from 8 subclasses to 2 main classes\n",
    "    n_class_label, m_class_label = tf.split(target_, [n_class, m_class], 1)\n",
    "    m_class_label1 =tf.reduce_sum(m_class_label,1, keepdims =True) \n",
    "    n_class_label1 =tf.reduce_sum(n_class_label,1, keepdims =True) \n",
    "    main_class_labels = tf.concat([n_class_label1, m_class_label1], 1)\n",
    "\n",
    "    #Aggregated cost\n",
    "    sub_class_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=target_))\n",
    "    main_class_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=main_class_logits, labels=main_class_labels))\n",
    "    total_cost = sub_class_cost + sub_scaling*main_class_cost\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(total_cost)\n",
    "    \n",
    "    #Accuracy value\n",
    "    subclass_accuracy = tf.equal(tf.argmax(logits, 1), tf.argmax(target_, 1))\n",
    "    subclass_accuracy =  tf.reduce_mean(tf.cast(subclass_accuracy, tf.float32), name='accuracy') #raw score \n",
    "    \n",
    "    main_class_accuracy = tf.equal(tf.argmax(main_class_logits, 1), tf.argmax(main_class_labels, 1))\n",
    "    main_class_accuracy = tf.reduce_mean(tf.cast(main_class_accuracy, tf.float32), name='accuracy') #raw score \n",
    "    \n",
    "    confidence_sub_class =  tf.reduce_sum(tf.multiply(subclass_softmax,target_),1)\n",
    "    confidence_main_class =  tf.reduce_sum(tf.multiply(main_class_softmax,main_class_labels),1)\n",
    "    \n",
    "    return optimizer,total_cost,subclass_softmax,main_class_softmax,subclass_accuracy,main_class_accuracy,confidence_sub_class,confidence_main_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "#### 12. Defining the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_o_input 3\n",
      "hidden size: 3\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_class = 3\n",
    "m_class = 5\n",
    "n_o_class = m_class+n_class\n",
    "input_ = tf.placeholder(tf.float32,  [None,n_o_input],name = 'x')\n",
    "target_ = tf.placeholder(tf.float32,[None,n_o_class],name='y')\n",
    "keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "\n",
    "epochs = 10\n",
    "keep_probability = 0.95\n",
    "sub_scaling = 1 \n",
    "optimizer,total_cost,subclass_softmax,main_class_softmax,subclass_accuracy,main_class_accuracy,confidence_sub_class,confidence_main_class =model_loss(input_,target_,m_class, n_class,n_o_input, keep_prob,relu_alpha = 0.2,sub_scaling = sub_scaling) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. Doing the actual training\n",
    "The neural network was modified based on the classification accuracy for the validation set. These modifications are made to find the \"optimal\" structure with the best classifcation performance.  \n",
    "- Prediction accuracy represents that % of readings in each batch whose subclass is correctly classified  \n",
    "- Prediction confidence represents the cross entropy score. In statistics, a prediction at a 95% confidence would mean that the prediction would be correct for 95% of the time where such confidence value was given.  While the networkâ€™s classification confidence doesnâ€™t directly translate to a statistical confidence, the confidence value of a  well trained neural network does exist as a good indication of future prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Validation Epoch  1 - Main Class Accuracy: 0.70000, , Main Class Confidence 0.51403, Subclass Accuracy: 0.30000, Subclass Confidence: 0.12722\n",
      "Validation Epoch  2 - Main Class Accuracy: 0.60000, , Main Class Confidence 0.51352, Subclass Accuracy: 0.20000, Subclass Confidence: 0.12700\n",
      "Validation Epoch  3 - Main Class Accuracy: 0.70000, , Main Class Confidence 0.53852, Subclass Accuracy: 0.20000, Subclass Confidence: 0.12826\n",
      "Validation Epoch  4 - Main Class Accuracy: 0.60000, , Main Class Confidence 0.52371, Subclass Accuracy: 0.40000, Subclass Confidence: 0.12857\n",
      "Validation Epoch  5 - Main Class Accuracy: 1.00000, , Main Class Confidence 0.63881, Subclass Accuracy: 0.30000, Subclass Confidence: 0.13701\n",
      "Validation Epoch  6 - Main Class Accuracy: 0.60000, , Main Class Confidence 0.53037, Subclass Accuracy: 0.30000, Subclass Confidence: 0.13106\n",
      "Validation Epoch  7 - Main Class Accuracy: 0.60000, , Main Class Confidence 0.53290, Subclass Accuracy: 0.10000, Subclass Confidence: 0.13097\n",
      "Validation Epoch  8 - Main Class Accuracy: 0.60000, , Main Class Confidence 0.53501, Subclass Accuracy: 0.20000, Subclass Confidence: 0.13248\n",
      "Validation Epoch  9 - Main Class Accuracy: 0.60000, , Main Class Confidence 0.53701, Subclass Accuracy: 0.20000, Subclass Confidence: 0.13185\n",
      "Validation Epoch 10 - Main Class Accuracy: 0.70000, , Main Class Confidence 0.57702, Subclass Accuracy: 0.20000, Subclass Confidence: 0.13419\n"
     ]
    }
   ],
   "source": [
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, label_pre_one_hot in get_batches(train_x, train_y,batch_size = 10):\n",
    "            batch_labels = one_hot_encode(label_pre_one_hot,n_o_class)\n",
    "            \n",
    "            optimizer_p= sess.run([optimizer], feed_dict = {input_:batch_features,target_:batch_labels,keep_prob:keep_probability})\n",
    "\n",
    "            batch_i += 1 \n",
    "        print('Validation Epoch {:>2} - '.format(epoch + 1), end='')\n",
    "        \n",
    "        random_index = random_no_repeat(0,len(val_x)-1,10)\n",
    "        valid_labels = one_hot_encode(val_y[random_index],n_o_class) \n",
    "        subclass_accuracy_p,main_class_accuracy_p,confidence_sub_class_p,confidence_main_class_p= sess.run([subclass_accuracy,main_class_accuracy,confidence_sub_class,confidence_main_class], feed_dict = {input_:val_x[random_index],target_:valid_labels,keep_prob:1})\n",
    "        print(\"Main Class Accuracy: {:.5f}, , Main Class Confidence {:.5f}, Subclass Accuracy: {:.5f}, Subclass Confidence: {:.5f}\".format(np.mean(main_class_accuracy_p),np.mean(confidence_main_class_p),np.mean(subclass_accuracy_p),np.mean(confidence_sub_class_p)))\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "#### 13. Testing the neural network\n",
    "When the modifications made were no longer improving the validation accuracy, we were satisfied that we had achieved the \"optimal\" structure. \n",
    "The network was then tested with the an unseen set of spectral data to confirm network's performance. \n",
    "In the validation phase, modifications were made based on the network's previous performance , where some of the improvements may be specific to the validation data set. \n",
    "In the testing phase, the use of unseen data produced accuracy values that are representative of real-world performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "INFO:tensorflow:Restoring parameters from ../training_rgb_sample\n",
      "Sample 1, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69270, Subclass Accuracy: 0.00000, Subclass Confidence: 0.14202\n",
      "Sample 2, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69271, Subclass Accuracy: 0.00000, Subclass Confidence: 0.14202\n",
      "Sample 3, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69257, Subclass Accuracy: 1.00000, Subclass Confidence: 0.14934\n",
      "Sample 4, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69267, Subclass Accuracy: 0.00000, Subclass Confidence: 0.14201\n",
      "Sample 5, Class 4,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30690, Subclass Accuracy: 0.00000, Subclass Confidence: 0.11856\n",
      "Sample 6, Class 1,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69264, Subclass Accuracy: 0.00000, Subclass Confidence: 0.13231\n",
      "Sample 7, Class 1,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69275, Subclass Accuracy: 0.00000, Subclass Confidence: 0.13294\n",
      "Sample 8, Class 4,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30537, Subclass Accuracy: 0.00000, Subclass Confidence: 0.11931\n",
      "Sample 9, Class 4,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30708, Subclass Accuracy: 0.00000, Subclass Confidence: 0.11832\n",
      "Sample 10, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69293, Subclass Accuracy: 1.00000, Subclass Confidence: 0.15113\n",
      "Sample 11, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69266, Subclass Accuracy: 0.00000, Subclass Confidence: 0.14205\n",
      "Sample 12, Class 1,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69297, Subclass Accuracy: 0.00000, Subclass Confidence: 0.13242\n",
      "Sample 13, Class 7,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30727, Subclass Accuracy: 0.00000, Subclass Confidence: 0.11088\n",
      "Sample 14, Class 4,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30576, Subclass Accuracy: 0.00000, Subclass Confidence: 0.11898\n",
      "Sample 15, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69256, Subclass Accuracy: 1.00000, Subclass Confidence: 0.14886\n",
      "Sample 16, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69363, Subclass Accuracy: 1.00000, Subclass Confidence: 0.15388\n",
      "Sample 17, Class 8,Main Class Accuracy: 0.00000, Main Class Confidence: 0.31248, Subclass Accuracy: 0.00000, Subclass Confidence: 0.10316\n",
      "Sample 18, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69344, Subclass Accuracy: 1.00000, Subclass Confidence: 0.15948\n",
      "Sample 19, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69331, Subclass Accuracy: 0.00000, Subclass Confidence: 0.14185\n",
      "Sample 20, Class 1,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69283, Subclass Accuracy: 0.00000, Subclass Confidence: 0.13270\n",
      "Sample 21, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69273, Subclass Accuracy: 0.00000, Subclass Confidence: 0.14200\n",
      "Sample 22, Class 8,Main Class Accuracy: 0.00000, Main Class Confidence: 0.31495, Subclass Accuracy: 0.00000, Subclass Confidence: 0.10035\n",
      "Sample 23, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69318, Subclass Accuracy: 1.00000, Subclass Confidence: 0.15123\n",
      "Sample 24, Class 5,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30680, Subclass Accuracy: 0.00000, Subclass Confidence: 0.12042\n",
      "Sample 25, Class 5,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30690, Subclass Accuracy: 0.00000, Subclass Confidence: 0.12051\n",
      "Sample 26, Class 6,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30686, Subclass Accuracy: 0.00000, Subclass Confidence: 0.11492\n",
      "Sample 27, Class 5,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30719, Subclass Accuracy: 0.00000, Subclass Confidence: 0.12061\n",
      "Sample 28, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69277, Subclass Accuracy: 1.00000, Subclass Confidence: 0.14983\n",
      "Sample 29, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69326, Subclass Accuracy: 0.00000, Subclass Confidence: 0.14178\n",
      "Sample 30, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69261, Subclass Accuracy: 1.00000, Subclass Confidence: 0.14901\n",
      "Sample 31, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69326, Subclass Accuracy: 0.00000, Subclass Confidence: 0.14179\n",
      "Sample 32, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69261, Subclass Accuracy: 1.00000, Subclass Confidence: 0.15015\n",
      "Sample 33, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69262, Subclass Accuracy: 1.00000, Subclass Confidence: 0.14907\n",
      "Sample 34, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69283, Subclass Accuracy: 0.00000, Subclass Confidence: 0.14197\n",
      "Sample 35, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69273, Subclass Accuracy: 0.00000, Subclass Confidence: 0.14203\n",
      "Sample 36, Class 1,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69282, Subclass Accuracy: 0.00000, Subclass Confidence: 0.13251\n",
      "Sample 37, Class 5,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30652, Subclass Accuracy: 0.00000, Subclass Confidence: 0.12033\n",
      "Sample 38, Class 1,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69283, Subclass Accuracy: 0.00000, Subclass Confidence: 0.13199\n",
      "Sample 39, Class 1,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69268, Subclass Accuracy: 0.00000, Subclass Confidence: 0.13322\n",
      "Sample 40, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69274, Subclass Accuracy: 0.00000, Subclass Confidence: 0.14202\n",
      "Sample 41, Class 5,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30696, Subclass Accuracy: 0.00000, Subclass Confidence: 0.12057\n",
      "Sample 42, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69307, Subclass Accuracy: 1.00000, Subclass Confidence: 0.15231\n",
      "Sample 43, Class 7,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30527, Subclass Accuracy: 0.00000, Subclass Confidence: 0.10947\n",
      "Sample 44, Class 4,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30519, Subclass Accuracy: 0.00000, Subclass Confidence: 0.11905\n",
      "Sample 45, Class 6,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30714, Subclass Accuracy: 0.00000, Subclass Confidence: 0.11451\n",
      "Sample 46, Class 1,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69335, Subclass Accuracy: 0.00000, Subclass Confidence: 0.13100\n",
      "Sample 47, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69281, Subclass Accuracy: 0.00000, Subclass Confidence: 0.14200\n"
     ]
    }
   ],
   "source": [
    "keep_probability = 1\n",
    "print('Testing...')\n",
    "with tf.Session() as sess:\n",
    "    loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "    loader.restore(sess, save_model_path)\n",
    "    \n",
    "    for epoch in range(1):\n",
    "        batch_i = 1\n",
    "        for batch_features, label_pre_one_hot in get_batches(val_x, val_y,batch_size = 1):# to be changed\n",
    "            batch_labels = one_hot_encode(label_pre_one_hot,n_o_class)      \n",
    "            \n",
    "            subclass_accuracy_p,main_class_accuracy_p,confidence_sub_class_p,confidence_main_class_p= sess.run([subclass_accuracy,main_class_accuracy,confidence_sub_class,confidence_main_class], feed_dict = {input_:batch_features,target_:batch_labels,keep_prob:1})\n",
    "            print('Sample {}, Class {},'.format(batch_i,label_pre_one_hot[0]), end='')\n",
    "            print(\"Main Class Accuracy: {:.5f}, Main Class Confidence: {:.5f}, Subclass Accuracy: {:.5f}, Subclass Confidence: {:.5f}\".format(np.mean(main_class_accuracy_p),np.mean(confidence_main_class_p),np.mean(subclass_accuracy_p),np.mean(confidence_sub_class_p)))\n",
    "\n",
    "            batch_i += 1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
